{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640e5831-d838-4cca-adea-f5409e1b2f2e",
   "metadata": {},
   "source": [
    "# CardiCat Dev v0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1598cc91-346c-41fa-b1ec-e774630813fe",
   "metadata": {},
   "source": [
    "CardiCat Only Walkthrough (both `ipynb` and `shell` compatible)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1cf2ab-6094-4152-8688-ffd3dd91d7df",
   "metadata": {},
   "source": [
    "High-cardinality categorical features are a common characteristic of mixed-type tabular datasets.   \n",
    "Existing generative model architectures struggle to learn the complexities of such data at scale,   \n",
    "primarily due to the difficulty of parameterizing the categorical features.   \n",
    "In this paper, we present a general variational autoencoder model, CardiCat, that can accurately fit  \n",
    " imbalanced high-cardinality and heterogeneous tabular data. Our method substitutes one-hot encoding  \n",
    "  with regularized dual encoder-decoder embedding layers, which are jointly learned.  \n",
    "   This approach enables us to use embeddings that depend also on the other covariates,   \n",
    "   leading to a compact and homogenized parameterization of categorical features.   \n",
    "   Our model employs a considerably smaller trainable parameter space than competing methods,   \n",
    "   enabling learning at a large scale. CardiCat generates high-quality synthetic data that   \n",
    "   better represent high-cardinality and imbalanced features compared to competing VAE models for multiple real and simulated datasets.   \n",
    "\n",
    "This walkthrough benchmarks CardiCat (or VAE , tVAE, tGAN, cCardiCatMask) using non-trivial datasets.  \n",
    "\n",
    "**Make sure the `params_dict.txt` is populated according to your specifications**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e8ccd0-bbfb-472c-9b16-eacf54a308ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T20:16:01.123722Z",
     "iopub.status.busy": "2023-02-18T20:16:01.078650Z",
     "iopub.status.idle": "2023-02-18T20:16:01.221625Z",
     "shell.execute_reply": "2023-02-18T20:16:01.216949Z",
     "shell.execute_reply.started": "2023-02-18T20:16:01.122994Z"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8fe8e5-9f6e-4f60-b18e-5c6af152c6c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from time import process_time\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from cycler import cycler\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Benchmarking & Evaluation:\n",
    "from sdv.tabular import CTGAN, TVAE\n",
    "\n",
    "sns.set_theme(style=\"ticks\", color_codes=True)\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "\n",
    "comp_name = \"\"\n",
    "param_dict_path = \"/Users/{}/Dropbox/School/gitRepos/CardiCat/params_dict.txt\".format(\n",
    "    comp_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6a05a5-7185-4cf6-b946-eed71761b156",
   "metadata": {},
   "source": [
    "When executing through the notebook, the cell below must be specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24addd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"CardiCat\"  # possible values: CardiCat, VAE (VAE-vanilla), eVAE (VAE-encoder)\n",
    "dataset = \"Credit\"  # possible values: PetFinder, Bank,Credit, Census, Medical,MIMIC,Criteo, Simulated\n",
    "run_id = \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc04279f-f201-4a91-8fc4-9c564e3696ca",
   "metadata": {},
   "source": [
    "When executing through shell as a py file, the cell below must be commented-out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459032f7-6d9e-4769-92a0-093cb96e0251",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comment-out if executed using shell!\n",
    "sys.argv = [0, model, dataset, param_dict_path]\n",
    "print(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8a8229-05f9-408f-97cc-a604e0c5eff7",
   "metadata": {},
   "source": [
    "NOTE: there is no need to change any of the code below this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56bf033-8886-4541-91ad-44fe349dfd94",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39e150-13fa-4ec5-bf06-7a5b65b39fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieving the user defined shell parameters:\n",
    "param_dict_path = sys.argv[3]\n",
    "# Reading the dictionary params from the dict file:\n",
    "with open(param_dict_path) as f:\n",
    "    params_tmp = f.read()\n",
    "param_dict = ast.literal_eval(params_tmp)\n",
    "# Adding user defined params to the params dictionary:\n",
    "param_dict[\"model\"] = sys.argv[1]\n",
    "param_dict[\"dataset_log_name\"] = sys.argv[2]\n",
    "param_dict[\"param_dict_path\"] = sys.argv[3]\n",
    "param_dict[\"run_id\"] = run_id\n",
    "\n",
    "# Setting  logical model contraints:\n",
    "if param_dict[\"model\"] == \"VAE\":\n",
    "    param_dict[\"embed_th\"] = 5000  # just a big threshold to avoid\n",
    "    param_dict[\"emb_loss\"] = False\n",
    "elif param_dict[\"model\"] == \"eVAE\":\n",
    "    param_dict[\"emb_loss\"] = False\n",
    "elif param_dict[\"model\"] == \"CardiCat\":\n",
    "    param_dict[\"emb_loss\"] = True\n",
    "    param_dict[\"onehot_ind_cat\"] = param_dict[\"onehot_ind_cat\"]\n",
    "else:\n",
    "    raise Exception(\"Sorry, model name not recognized\")\n",
    "\n",
    "intro = \"\"\"\\n\n",
    "########EXECUTING CARDICAT : {} WITH THE FOLLOWING:########\n",
    "\"\"\".format(param_dict[\"model\"])\n",
    "print(intro)\n",
    "print(\"var1 model:\", sys.argv[1])\n",
    "print(\"var2 dataset_log_name:\", sys.argv[2])\n",
    "print(\"var3 latent_dim:\", param_dict[\"latent_dim\"])\n",
    "print(\"     param file path:\", param_dict_path)\n",
    "print(\"var4 CardiCat library path:\", param_dict[\"lib_path\"])\n",
    "print(\"var5 CardiCat data path:\", param_dict[\"data_path\"])\n",
    "# Copy param_dict to add to logs\n",
    "logs = param_dict.copy()\n",
    "logs[\"date_time\"] = datetime.now().strftime(\"%Y%m%d_%H%M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07294a38-2b1b-4762-b6d1-6f67c9f32cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the CardiCat module:\n",
    "sys.path.insert(1, param_dict[\"lib_path\"])\n",
    "\n",
    "from src import network as network\n",
    "from src import postprocessing as postprocessing\n",
    "from src import preprocessing as preprocessing\n",
    "from src import reporting as reporting\n",
    "from src import vae_model as vae\n",
    "\n",
    "if not param_dict[\"show_figs\"]:\n",
    "    plt.ioff()\n",
    "print(\"Logs:\\n\", logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f37d53-121e-4565-b22e-ecf97c26c8b5",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0928b0-9710-48c4-9628-e02ccc0d61be",
   "metadata": {},
   "source": [
    "Loading Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb3f97-af34-46a3-a9aa-16301ee96f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a table of the cardinality and type for each vairable:\n",
    "## Making a copy of the dataset object to be able to compare the synthetic data\n",
    "## to real data at the end of the notebook:\n",
    "dataframe, is_target = preprocessing.load_dataset(\n",
    "    param_dict[\"dataset_log_name\"], param_dict[\"data_path\"]\n",
    ")\n",
    "# if param_dict[\"dataset_log_name\"]==\"Simulated\":\n",
    "#     dataframe = dataframe.sample(20000)\n",
    "param_dict[\"is_target\"] = is_target\n",
    "dataframe, dataframe_test = np.split(\n",
    "    dataframe.sample(frac=1), [int(0.7 * len(dataframe))]\n",
    ")\n",
    "print(\"Train data size: \", dataframe.shape, \"Test data size: \", dataframe_test.shape)\n",
    "df = dataframe.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9d895-6121-4ffa-83f4-3101142d5d04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get cardinality and type per feature:\n",
    "preprocessing.get_df_cardinality(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0936694b-783f-4450-ab04-55b3be23a33c",
   "metadata": {},
   "source": [
    "Detecting column types for preprocesing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785074e5-91da-4644-bfbe-9f65d3866665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating lists of categorica,integer, and float columns (features/variables):\n",
    "catCols, intCols, floatCols = preprocessing.get_col_types(\n",
    "    df, is_y=param_dict[\"is_target\"], verbose=True\n",
    ")\n",
    "col_tokens_all_tmp = {}\n",
    "for i in catCols:\n",
    "    col_tokens_all_tmp[i] = len(df[i].unique())\n",
    "\n",
    "if param_dict[\"cVAE\"]:\n",
    "    tmpEmb = [\n",
    "        key\n",
    "        for key, value in col_tokens_all_tmp.items()\n",
    "        if value >= param_dict[\"embed_th\"]\n",
    "    ]\n",
    "else:\n",
    "    tmpEmb = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the marginal probabilities of each categorical feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catMarginals = preprocessing.get_catMarginals(dataframe, catCols)\n",
    "# catMarginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=len(catMarginals), figsize=(10, 2))\n",
    "for i, key in enumerate(catMarginals):\n",
    "    pd.DataFrame.from_dict(catMarginals[key], orient=\"index\")[:20].plot.bar(\n",
    "        ax=axes[i], legend=False\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional VAE or MASK logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if param_dict[\"cVAE\"]:\n",
    "    # marg_sample = lambda e: np.random.choice(\n",
    "    #     list(catMarginals[e].keys()), p=list(catMarginals[e].values())\n",
    "    #\n",
    "    alpha = 1  # (if <= alpha then x, else marg sample)\n",
    "\n",
    "    if param_dict[\"cVAE_mask\"]:\n",
    "        for col in tmpEmb:\n",
    "            df[\"cond_\" + col] = \"mask\"\n",
    "        for index, row in df.iterrows():\n",
    "            # print(index,row)\n",
    "            col_sampled = np.random.choice(tmpEmb)\n",
    "            # df.loc[index, \"cond_\" + col_sampled] = preprocessing.marg_sample(\n",
    "            #     col_sampled, catMarginals\n",
    "            # )\n",
    "\n",
    "            # df.loc[index, \"cond_\" + col_sampled] = df.loc[index, col_sampled]\n",
    "            df.loc[index, \"cond_\" + col_sampled] = (\n",
    "                df.loc[index, col_sampled]\n",
    "                if np.random.rand() <= alpha\n",
    "                else preprocessing.marg_sample(col_sampled, catMarginals)\n",
    "            )\n",
    "    else:\n",
    "        for col in tmpEmb:\n",
    "            df[\"cond_\" + col] = dataframe[col].apply(\n",
    "                lambda x: x\n",
    "                if np.random.rand() <= alpha\n",
    "                else preprocessing.marg_sample(col, catMarginals)\n",
    "            )  # df[col].copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate CramersV for reporting purposes of orignal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "anove_mixed_original = reporting.get_mixed_anova(\n",
    "    catCols, intCols, floatCols, dataframe_test\n",
    ")\n",
    "cramersV_original = reporting.get_catCols_cramersV(catCols, dataframe_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/test split tf.dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting into train/test:\n",
    "# # this is to avoid the bug with tf.dataset:\n",
    "\n",
    "# ### Label encoding (ordinal encoding) the categorical variabels:\n",
    "df, label_encoder = preprocessing.labelEncoding(\n",
    "    df,\n",
    "    catCols,\n",
    ")\n",
    "\n",
    "if param_dict[\"cVAE_mask\"]:\n",
    "    preprocessing.add_mask_labels(label_encoder)\n",
    "\n",
    "if param_dict[\"cVAE\"]:\n",
    "    for col in tmpEmb:\n",
    "        df[\"cond_\" + col] = label_encoder[col].transform(df[\"cond_\" + col])\n",
    "\n",
    "\n",
    "if param_dict[\"dataset_log_name\"] == \"Criteo\":\n",
    "    tst = np.asarray(df).astype(\"int64\")\n",
    "    tst = pd.DataFrame(tst, columns=df.columns)\n",
    "    train, test = np.split(tst.sample(frac=1), [int(0.8 * len(tst))])\n",
    "    print(\"length of train:\", len(train))\n",
    "    print(\"length of test:\", len(test))\n",
    "    train_ds = preprocessing.df_to_dset(\n",
    "        train, is_y=param_dict[\"is_target\"], batch_size=param_dict[\"batch_size\"]\n",
    "    )\n",
    "    test_ds = preprocessing.df_to_dset(\n",
    "        test, is_y=param_dict[\"is_target\"], batch_size=param_dict[\"batch_size\"]\n",
    "    )\n",
    "    train_ds.element_spec\n",
    "else:\n",
    "    train, test = np.split(\n",
    "        df.sample(frac=1), [int(param_dict[\"train_ratio\"] * len(df))]\n",
    "    )\n",
    "    print(\"Total length: {:,}\".format(len(df)))\n",
    "    print(\n",
    "        \"length of train: {:,} which is {:.1%}\".format(\n",
    "            len(train), param_dict[\"train_ratio\"]\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"length of test: {:,} which is {:.1%}\".format(\n",
    "            len(test), 1 - param_dict[\"train_ratio\"]\n",
    "        )\n",
    "    )\n",
    "    train_ds = preprocessing.df_to_dset(\n",
    "        train, is_y=param_dict[\"is_target\"], batch_size=param_dict[\"batch_size\"]\n",
    "    )\n",
    "    test_ds = preprocessing.df_to_dset(\n",
    "        test, is_y=param_dict[\"is_target\"], batch_size=param_dict[\"batch_size\"]\n",
    "    )\n",
    "    train_ds.element_spec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8741e29-11db-407e-a5ff-abd2deeedf55",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoder = preprocessing.data_encoder(\n",
    "    original_df=dataframe, train_ds=train_ds, param_dict=param_dict\n",
    ")\n",
    "print(data_encoder.catCols)\n",
    "print(\"Categorical column tokens (cardinality): \", data_encoder.col_tokens_all)\n",
    "# emb_sizes = data_encoder.emb_sizes\n",
    "print(\"Total Cardinality: {:,}\".format(sum(data_encoder.col_tokens_all.values())))\n",
    "print(\"Suggested embedding size for each cat column: \", data_encoder.emb_sizes_all)\n",
    "print(\"embCols: \", data_encoder.embCols)\n",
    "print(\"ohCols: \", data_encoder.ohCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749a6317-c825-4504-b447-f846844b0761",
   "metadata": {},
   "source": [
    "## Defining Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if param_dict[\"cVAE\"]:\n",
    "    all_features_cond = tf.keras.layers.concatenate(\n",
    "        [\n",
    "            data_encoder.all_features,\n",
    "            tf.keras.layers.concatenate(data_encoder.condFeatures),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# all_features_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decayed_learning_rate(step):\n",
    "    return initial_learning_rate * decay_rate ** (step / decay_steps)\n",
    "\n",
    "\n",
    "initial_learning_rate = param_dict[\"learn_rate\"]\n",
    "decay_rate = 0.5\n",
    "decay_steps = 50\n",
    "print(\"{:.6f}\".format(decayed_learning_rate(150)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060d567-b63f-4d03-a552-f6230921a8eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Complete Network:\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=param_dict[\"learn_rate\"])\n",
    "# optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=lr_schedule)\n",
    "final = vae.sampling(param_dict[\"latent_dim\"], param_dict[\"latent_dim\"])\n",
    "input_decoder = param_dict[\"latent_dim\"]\n",
    "\n",
    "\n",
    "# INCOMPLETE!! (probably only for conditional)\n",
    "if param_dict[\"cVAE\"]:\n",
    "    dec_inputs_tmp = [\n",
    "        tf.keras.layers.Input(shape=input_decoder, name=\"dec_latent_input\")\n",
    "    ]\n",
    "    dec_inputs = dec_inputs_tmp + data_encoder.condInputs\n",
    "    dec_features = dec_inputs_tmp + data_encoder.condFeatures\n",
    "    dec_features = tf.keras.layers.concatenate(dec_features)\n",
    "\n",
    "    enc = network.encoder(\n",
    "        data_encoder.all_inputs + data_encoder.condInputs,\n",
    "        all_features_cond,\n",
    "        param_dict[\"latent_dim\"],\n",
    "    )\n",
    "    dec = network.decoder(dec_inputs, dec_features, data_encoder.layer_sizes)\n",
    "    # dec_inputs = tf.keras.Input(shape=input_decoder, name='decoder_input_layer')\n",
    "    # dec = network.decoder(dec_inputs,dec_inputs,data_encoder.layer_sizes)\n",
    "else:\n",
    "    dec_inputs = tf.keras.Input(shape=input_decoder, name=\"dec_latent_input\")\n",
    "    enc = network.encoder(\n",
    "        data_encoder.all_inputs, data_encoder.all_features, param_dict[\"latent_dim\"]\n",
    "    )\n",
    "    dec = network.decoder(dec_inputs, dec_inputs, data_encoder.layer_sizes)\n",
    "\n",
    "# dec = network.decoder(input_decoder,data_encoder.ohCols,param_dict,oh_tokens,emb_tokens,\n",
    "#                       col_tokens_all.values(),embCols,emb_sizes,intCols,\n",
    "#                       floatCols,layer_sizes)\n",
    "\n",
    "if param_dict[\"emb_loss\"]:\n",
    "    if param_dict[\"cVAE\"]:\n",
    "        cod = vae.codex(\n",
    "            data_encoder.all_inputs + data_encoder.condInputs, data_encoder.all_features\n",
    "        )\n",
    "    else:\n",
    "        # cod = vae.codex(data_encoder.all_inputs,data_encoder.all_features)\n",
    "        cod = vae.codex(data_encoder.all_inputs, data_encoder.all_features_cod)\n",
    "else:\n",
    "    cod = vae.codex(data_encoder.all_inputs_1hot, data_encoder.all_features_1hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cod.summary()\n",
    "# for l in cod.layers:\n",
    "#     print(l.name, l.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bbdaa1-3b70-42f3-ad5e-f6c39986e1da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment only if your system supports graphviz\n",
    "#\n",
    "# Save network architecture plots:\n",
    "if param_dict[\"save_figs\"]:\n",
    "    tf.keras.utils.plot_model(\n",
    "        enc,\n",
    "        show_shapes=True,\n",
    "        rankdir=\"LR\",\n",
    "        to_file=os.path.join(\n",
    "            param_dict[\"lib_path\"],\n",
    "            \"output/model_architecture_graphs/{}_{}_encoder.png\".format(\n",
    "                logs[\"date_time\"], logs[\"dataset_log_name\"]\n",
    "            ),\n",
    "        ),\n",
    "        show_layer_activations=True,\n",
    "    )\n",
    "\n",
    "    tf.keras.utils.plot_model(\n",
    "        dec,\n",
    "        show_shapes=True,\n",
    "        rankdir=\"LR\",\n",
    "        to_file=os.path.join(\n",
    "            param_dict[\"lib_path\"],\n",
    "            \"output/model_architecture_graphs/{}_{}_decoder.png\".format(\n",
    "                logs[\"date_time\"], logs[\"dataset_log_name\"]\n",
    "            ),\n",
    "        ),\n",
    "        show_layer_activations=True,\n",
    "    )\n",
    "\n",
    "    tf.keras.utils.plot_model(\n",
    "        cod,\n",
    "        show_shapes=True,\n",
    "        rankdir=\"LR\",\n",
    "        to_file=os.path.join(\n",
    "            param_dict[\"lib_path\"],\n",
    "            \"output/model_architecture_graphs/{}_{}_codex.png\".format(\n",
    "                logs[\"date_time\"], logs[\"dataset_log_name\"]\n",
    "            ),\n",
    "        ),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7829d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    enc,\n",
    "    show_shapes=True,\n",
    "    rankdir=\"LR\",\n",
    "    show_layer_activations=True,\n",
    "    show_trainable=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1aa8b2-6c4e-47df-a2cc-385d03f63a52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get number of network's trainable parameters;\n",
    "trainableParamsEnc = np.sum([np.prod(v.shape) for v in enc.trainable_weights])\n",
    "trainableParamsDec = np.sum([np.prod(v.shape) for v in dec.trainable_weights])\n",
    "\n",
    "print(\"Traniable Parameters Encoder: {:,}\".format(int(trainableParamsEnc)))\n",
    "print(\"Traniable Parameters Decoder: {:,}\".format(int(trainableParamsDec)))\n",
    "print(\n",
    "    \"Total-Traniable Parameters VAE: {:,}\".format(\n",
    "        int(trainableParamsEnc + trainableParamsDec)\n",
    "    )\n",
    ")\n",
    "logs[\"trainable_params\"] = \"{:,}\".format(int(trainableParamsEnc + trainableParamsDec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2722d2",
   "metadata": {},
   "source": [
    "## CardiCat Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_weights = {\n",
    "    weights.name.split(\"/\")[0]: weights.numpy()\n",
    "    for weights in enc.weights\n",
    "    if weights.name.split(\"_\")[0] == \"emb\"\n",
    "}\n",
    "#\n",
    "emb_init_var = [\n",
    "    tf.math.reduce_mean(tf.math.reduce_variance(emb_weights[emb], axis=0))\n",
    "    for emb in emb_weights.keys()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434d6c2a-4a3d-4a78-bed8-afb437790d29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CardiCat_start = process_time()\n",
    "tf.config.run_functions_eagerly(True)\n",
    "output_loss, loss_logs_all, emb_weights = vae.train_high_vae(\n",
    "    train_ds=train_ds,\n",
    "    enc=enc,\n",
    "    dec=dec,\n",
    "    cod=cod,\n",
    "    final=final,\n",
    "    optimizer=optimizer,\n",
    "    layer_sizes=data_encoder.layer_sizes,\n",
    "    param_dict=param_dict,\n",
    "    weights=data_encoder.weights,\n",
    "    emb_init_var=emb_init_var,\n",
    "    tmpEmb=tmpEmb,\n",
    ")\n",
    "CardiCat_stop = process_time()\n",
    "CardiCat_time = round(CardiCat_stop - CardiCat_start, 2)\n",
    "print(\"Elapsed time in seconds:\", CardiCat_time)\n",
    "logs[\"losses\"] = round(output_loss.iloc[-1, 1:], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c72bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"one-hot columns: \", data_encoder.ohCols)\n",
    "print(\"embedded columns: \", data_encoder.embCols)\n",
    "print(\"numerical columns: \", data_encoder.numCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1a15ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_loss_df = pd.DataFrame(\n",
    "    tf.reshape(\n",
    "        loss_logs_all,\n",
    "        shape=(\n",
    "            param_dict[\"epochs\"],\n",
    "            len(\n",
    "                [*data_encoder.ohCols.values()]\n",
    "                + [*data_encoder.embCols.values()]\n",
    "                + data_encoder.numCols\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    columns=[*data_encoder.ohCols.keys()]\n",
    "    + [*data_encoder.embCols.keys()]\n",
    "    + data_encoder.numCols,\n",
    ")\n",
    "feature_loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f492cc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_loss = px.line(feature_loss_df, title=\"loss progression per feautre\")\n",
    "feature_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613e81a1-df43-4ebf-a7f7-5882bc35becf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453209a5-8f96-4e49-ac8d-5b2d502642d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "if param_dict[\"mixed_loss\"]:\n",
    "    fig, [ax1, ax2] = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n",
    "    df_melt = output_loss[\n",
    "        [\"epoch\", \"kl_loss\", \"mixed_loss_factor\", \"total_loss\", \"emb_reg_loss\"]\n",
    "    ].melt(id_vars=[\"epoch\"], var_name=\"loss_type\", value_name=\"loss\")\n",
    "    sns.lineplot(data=df_melt, x=\"epoch\", y=\"loss\", hue=\"loss_type\", ax=ax1)\n",
    "\n",
    "    df_melt = output_loss[\n",
    "        [\"epoch\", \"hot_loss\", \"emb_loss\", \"num_loss\", \"mixed_loss\", \"emb_reg_loss\"]\n",
    "    ].melt(id_vars=[\"epoch\"], var_name=\"loss_type\", value_name=\"loss\")\n",
    "    sns.lineplot(data=df_melt, x=\"epoch\", y=\"loss\", hue=\"loss_type\", ax=ax2)\n",
    "    fig\n",
    "\n",
    "else:\n",
    "    df_melt = output_loss[\n",
    "        [\"epoch\", \"total_loss\", \"mse_loss_factor\", \"kl_loss\", \"emb_reg_loss\"]\n",
    "    ].melt(id_vars=[\"epoch\"], var_name=\"loss_type\", value_name=\"loss\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    custom_cycler = (\n",
    "        cycler(color=[\"r\", \"k\", \"y\"])\n",
    "        + cycler(linestyle=[\"--\", \"-.\", \":\"])\n",
    "        + cycler(lw=[2, 1, 3])\n",
    "    )\n",
    "    ax.set_prop_cycle(custom_cycler)\n",
    "    df_melt.set_index([\"loss_type\", \"epoch\"]).unstack(\"loss_type\")[\"loss\"].plot(ax=ax)\n",
    "    ax.set_ylabel(\"loss\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_title(\"CardiCat Training Loss per Epic\")\n",
    "# fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1dba6f-7bf8-42a0-9e76-3753593f64a4",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean, log_var = enc.predict(train_ds)\n",
    "# latents = final([mean, log_var])\n",
    "# # dec.predict(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_tokens = list(data_encoder.ohCols.values())\n",
    "emb_sizes = list(data_encoder.embCols.values())\n",
    "embCols = list(data_encoder.embCols.keys())\n",
    "numCols = data_encoder.numCols\n",
    "numFeatures = data_encoder.numFeatures\n",
    "numLookup = data_encoder.numLookup\n",
    "# intCols = data_encoder.intCols\n",
    "all_inputs = data_encoder.all_inputs\n",
    "all_inputs_1hot = data_encoder.all_inputs_1hot\n",
    "col_tokens_all = data_encoder.col_tokens_all\n",
    "ohCols = list(data_encoder.ohCols.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5be6be-32fa-4d08-bb48-005fd7f2c943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## Evaluation\n",
    "\n",
    "# ### Predicting on the train set\n",
    "mean, log_var = enc.predict(train_ds)\n",
    "latents = final([mean, log_var])\n",
    "if param_dict[\"cVAE\"]:\n",
    "    df_y_dict = dict(train)\n",
    "    df_y_dict[\"dec_latent_input\"] = latents\n",
    "else:\n",
    "    df_y_dict = latents\n",
    "\n",
    "# create a dataframe of synthetic data, get the categorical embeddings:\n",
    "gen_df_train, emb_weights = postprocessing.get_pred(\n",
    "    enc,\n",
    "    dec,\n",
    "    df_y_dict,\n",
    "    param_dict,\n",
    "    oh_tokens,\n",
    "    emb_sizes,\n",
    "    embCols,\n",
    "    numFeatures,\n",
    "    catCols,\n",
    "    numCols,\n",
    "    numLookup,\n",
    "    intCols,\n",
    "    all_inputs,\n",
    "    # all_inputs_1hot,\n",
    "    list(col_tokens_all.values()),\n",
    "    label_encoder,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4382930c-2aa6-440f-85e5-41fc06c3d0a8",
   "metadata": {},
   "source": [
    "Train Set Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e43d8b-8fe6-48f0-a4a2-f4be3cac11eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transforme normalized and encoded splits to original state:\n",
    "train_decoded = postprocessing.decode_splits(train, label_encoder)\n",
    "test_decoded = postprocessing.decode_splits(test, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c93a69-64cc-406e-bfe7-89b07ad0d24d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "report_cardicat_train = postprocessing.get_report(\n",
    "    dataframe_test, gen_df_train, param_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f940639-aba7-48df-957d-d36f63d1d4ac",
   "metadata": {},
   "source": [
    "Random Sample out of Learned Prior Set Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting on random set\n",
    "n = dataframe_test.shape[0]  # len(dataframe)\n",
    "mean_rand = tf.reshape(\n",
    "    tf.tile(\n",
    "        enc.weights[-3],\n",
    "        [\n",
    "            n,\n",
    "        ],\n",
    "    ),\n",
    "    (n, param_dict[\"latent_dim\"]),\n",
    ")\n",
    "logvar_rand = tf.reshape(\n",
    "    tf.tile(\n",
    "        enc.weights[-1],\n",
    "        [\n",
    "            n,\n",
    "        ],\n",
    "    ),\n",
    "    (n, param_dict[\"latent_dim\"]),\n",
    ")\n",
    "\n",
    "cardicat_gen_rand = vae.sampling_model([mean_rand, logvar_rand])\n",
    "\n",
    "if param_dict[\"cVAE\"]:\n",
    "    df_y_rand = pd.DataFrame()\n",
    "    # marg_sample = lambda e: np.random.choice(list(catMarginals[e].keys()),p = list(catMarginals[e].values()))\n",
    "    alpha = 0  # we want only random sample from marg prob\n",
    "    # tmpEmb = ['Breed1', 'Color1', 'Color2', 'MaturitySize', 'FurLength', 'Vaccinated', 'Sterilized', 'Health']\n",
    "    if param_dict[\"cVAE_mask\"]:\n",
    "        for col in tmpEmb:\n",
    "            df_y_rand[\"cond_\" + col] = dataframe_test[col].apply(lambda x: \"mask\")\n",
    "        for index, row in df_y_rand.iterrows():\n",
    "            # print(index,row)\n",
    "            col_sampled = np.random.choice(tmpEmb)\n",
    "            df_y_rand.loc[index, \"cond_\" + col_sampled] = preprocessing.marg_sample(\n",
    "                col_sampled, catMarginals\n",
    "            )\n",
    "        for col in tmpEmb:\n",
    "            df_y_rand[\"cond_\" + col] = label_encoder[col].transform(\n",
    "                df_y_rand[\"cond_\" + col]\n",
    "            )\n",
    "    else:  # no mask\n",
    "        for col in tmpEmb:\n",
    "            df_y_rand[\"cond_\" + col] = dataframe_test[col].apply(\n",
    "                lambda x: x\n",
    "                if np.random.rand() <= alpha\n",
    "                else preprocessing.marg_sample(col, catMarginals)\n",
    "            )  # df[col].copy()\n",
    "            df_y_rand[\"cond_\" + col] = label_encoder[col].transform(\n",
    "                df_y_rand[\"cond_\" + col]\n",
    "            )\n",
    "    df_y_rand_dict = dict(df_y_rand)\n",
    "else:\n",
    "    df_y_rand_dict = {}\n",
    "\n",
    "\n",
    "df_y_rand_dict[\"dec_latent_input\"] = cardicat_gen_rand\n",
    "# df_y_rand_ds = tmp1\n",
    "# df_y_rand_ds = tf.data.Dataset.from_tensor_slices(tmp1)#.batch(param_dict['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46567a07-fef3-4c73-9ef6-b415bba4a8a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cardicat_gen_rand, trash = postprocessing.get_pred(\n",
    "    enc,\n",
    "    dec,\n",
    "    df_y_rand_dict,\n",
    "    param_dict,\n",
    "    oh_tokens,\n",
    "    emb_sizes,\n",
    "    embCols,\n",
    "    numFeatures,\n",
    "    catCols,\n",
    "    numCols,\n",
    "    numLookup,\n",
    "    intCols,\n",
    "    all_inputs,\n",
    "    # all_inputs_1hot,\n",
    "    list(col_tokens_all.values()),\n",
    "    label_encoder,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185df501-796d-43b8-8ceb-2123106dd4ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "report_cardicat_rand = postprocessing.get_report(\n",
    "    dataframe_test, cardicat_gen_rand, param_dict, full=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add98b14-fc1b-48db-ae3f-388423c74573",
   "metadata": {},
   "source": [
    "Logging Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anove_mixed_cardicat = reporting.get_mixed_anova(\n",
    "    catCols, intCols, floatCols, cardicat_gen_rand\n",
    ")\n",
    "qScore_mixed_cardicat, mixed_cardicat = reporting.get_qScoreMixed(\n",
    "    anove_mixed_original, anove_mixed_cardicat\n",
    ")\n",
    "ks_mixed_0_weighted, ks_mixed_1_weighted, ks_mixed_0, ks_mixed_1, ks_mixed_raw_stats = (\n",
    "    reporting.get_mean_ks_mixed_stats(\n",
    "        cardicat_gen_rand,\n",
    "        dataframe_test,\n",
    "        data_encoder.catCols,\n",
    "        data_encoder.intCols,\n",
    "        data_encoder.floatCols,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "cramersV_cardicat = reporting.get_catCols_cramersV(catCols, cardicat_gen_rand)\n",
    "score_cat_pairs_cardicat, pairs_cardicat = reporting.get_qScoreMixed(\n",
    "    cramersV_original, cramersV_cardicat\n",
    ")\n",
    "\n",
    "CardiCat_scores = {\n",
    "    \"marginals_all\": round(report_cardicat_rand[\"summary\"].iloc[0, 1], 2),\n",
    "    \"pairs_all\": round(report_cardicat_rand[\"summary\"].iloc[1, 1], 2),\n",
    "    \"marginals_KS\": round(\n",
    "        report_cardicat_rand[\"marginals\"][\n",
    "            report_cardicat_rand[\"marginals\"].Metric == \"KSComplement\"\n",
    "        ][\"Quality Score\"].mean(),\n",
    "        2,\n",
    "    ),\n",
    "    \"marginals_TV\": round(\n",
    "        report_cardicat_rand[\"marginals\"][\n",
    "            report_cardicat_rand[\"marginals\"].Metric == \"TVComplement\"\n",
    "        ][\"Quality Score\"].mean(),\n",
    "        2,\n",
    "    ),\n",
    "    \"pairs_corr\": round(\n",
    "        report_cardicat_rand[\"pairs\"][\n",
    "            report_cardicat_rand[\"pairs\"].Metric == \"CorrelationSimilarity\"\n",
    "        ][\"Quality Score\"].mean(),\n",
    "        2,\n",
    "    ),\n",
    "    \"pairs_cont\": round(\n",
    "        report_cardicat_rand[\"pairs\"][\n",
    "            report_cardicat_rand[\"pairs\"].Metric == \"ContingencySimilarity\"\n",
    "        ][\"Quality Score\"].mean(),\n",
    "        2,\n",
    "    ),\n",
    "    \"pairs_cont_fix\": round(\n",
    "        report_cardicat_rand[\"pairs\"][\n",
    "            (report_cardicat_rand[\"pairs\"][\"Column 1\"].isin(catCols))\n",
    "            & (report_cardicat_rand[\"pairs\"][\"Column 2\"].isin(catCols))\n",
    "        ][report_cardicat_rand[\"pairs\"].Metric == \"ContingencySimilarity\"][\n",
    "            \"Quality Score\"\n",
    "        ].mean(),\n",
    "        2,\n",
    "    ),\n",
    "    \"pairs_cat\": round(score_cat_pairs_cardicat, 2),\n",
    "    \"pairs_mixed\": round(qScore_mixed_cardicat, 2),\n",
    "    \"pairs_ks_mixed_0_weighted\": round(np.mean(list(ks_mixed_0_weighted.values())), 3),\n",
    "    \"pairs_ks_mixed_1_weighted\": round(np.mean(list(ks_mixed_1_weighted.values())), 3),\n",
    "    \"pairs_ks_mixed_0\": round(np.mean(list(ks_mixed_0.values())), 3),\n",
    "    \"pairs_ks_mixed_1\": round(np.mean(list(ks_mixed_0.values())), 3),\n",
    "}\n",
    "\n",
    "\n",
    "kys = [\n",
    "    \"loss_type\",\n",
    "    \"oh_loss_fun\",\n",
    "    \"embed_th\",\n",
    "    \"epochs\",\n",
    "    \"batch_size\",\n",
    "    \"learn_rate\",\n",
    "    \"recon_factor\",\n",
    "    \"emb_regularization\",\n",
    "    \"emb_reg_factor\",\n",
    "    \"hot_factor\",\n",
    "    \"emb_factor\",\n",
    "    \"num_factor\",\n",
    "    \"mse_factor\",\n",
    "    \"dataset_log_name\",\n",
    "    \"latent_dim\",\n",
    "    \"weighted_loss\",\n",
    "    \"emb_loss\",\n",
    "    \"date_time\",\n",
    "    \"trainable_params\",\n",
    "    \"onehot_ind_cat\",\n",
    "    \"comment\",\n",
    "    \"run_id\",\n",
    "]\n",
    "CardiCat_output = {k: v for k, v in logs.items() if k in kys}\n",
    "CardiCat_output[\"col_tokens_all\"] = col_tokens_all\n",
    "CardiCat_output[\"emb_sizes\"] = emb_sizes\n",
    "CardiCat_output[\"numCols\"] = numCols\n",
    "CardiCat_output[\"catCols\"] = catCols\n",
    "CardiCat_output[\"ohCols\"] = ohCols\n",
    "if param_dict[\"cVAE\"]:\n",
    "    if param_dict[\"cVAE_mask\"]:\n",
    "        CardiCat_output[\"model\"] = \"cCardiCatMask\"\n",
    "    else:\n",
    "        CardiCat_output[\"model\"] = \"cCardiCat\"\n",
    "else:\n",
    "    CardiCat_output[\"model\"] = param_dict[\"model\"]\n",
    "CardiCat_output[\"score_train\"] = round(report_cardicat_train, 2)\n",
    "CardiCat_output[\"score_rand\"] = round(report_cardicat_rand[\"summary\"].Score.mean(), 2)\n",
    "CardiCat_output[\"scores\"] = CardiCat_scores\n",
    "CardiCat_output[\"marginals\"] = report_cardicat_rand[\"marginals\"][\n",
    "    [\"Column\", \"Quality Score\"]\n",
    "].to_dict()\n",
    "CardiCat_output[\"pairs\"] = report_cardicat_rand[\"pairs\"][\n",
    "    [\"Column 1\", \"Column 2\", \"Metric\", \"Quality Score\"]\n",
    "].to_dict()\n",
    "CardiCat_output[\"cramersV\"] = pairs_cardicat.to_dict()\n",
    "CardiCat_output[\"mixed\"] = mixed_cardicat.to_dict()\n",
    "CardiCat_output[\"mixed_ks\"] = ks_mixed_raw_stats\n",
    "CardiCat_output[\"time\"] = CardiCat_time\n",
    "print(CardiCat_output[\"score_rand\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardicat_scores_df = pd.DataFrame(\n",
    "    CardiCat_output[\"scores\"].values(),\n",
    "    index=CardiCat_output[\"scores\"].keys(),\n",
    "    columns=[\"score\"],\n",
    ")\n",
    "display(cardicat_scores_df)\n",
    "px.bar(\n",
    "    cardicat_scores_df,\n",
    "    text_auto=\".2f\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REsults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if param_dict[\"attention\"]:\n",
    "    CardiCat_output[\"model\"] = \"CardiCatAttention\"\n",
    "\n",
    "# CardiCat_output[\"dataset_log_name\"] = \"Simulated20k\"\n",
    "# logs[\"dataset_log_name\"] = \"Simulated20k\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe4297f-9241-453b-8c28-3c0bd887dc7a",
   "metadata": {},
   "source": [
    "Writing logs to output folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2eb18a-42cb-4206-be62-0be7fdc723fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if param_dict[\"save_files\"]:\n",
    "    cardicat_gen_rand.to_pickle(\n",
    "        os.path.join(\n",
    "            param_dict[\"lib_path\"],\n",
    "            \"{}/synthetics/synthetics_pkl_{}_{}_{}\".format(\n",
    "                param_dict[\"output_path\"],\n",
    "                logs[\"dataset_log_name\"],\n",
    "                logs[\"date_time\"],\n",
    "                CardiCat_output[\"model\"] + \"_rand\",\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "outlog_cardicat = pd.DataFrame.from_dict(\n",
    "    {key: [CardiCat_output[key]] for key in CardiCat_output}\n",
    ")\n",
    "if param_dict[\"save_files\"]:\n",
    "    outlog_cardicat.to_csv(\n",
    "        param_dict[\"lib_path\"]\n",
    "        + \"{}/outlogs/{}_{}_{}\".format(\n",
    "            param_dict[\"output_path\"],\n",
    "            logs[\"date_time\"],\n",
    "            CardiCat_output[\"model\"],\n",
    "            logs[\"dataset_log_name\"],\n",
    "        )\n",
    "    )\n",
    "    report_cardicat_rand[\"marginals\"].to_csv(\n",
    "        param_dict[\"lib_path\"]\n",
    "        + \"{}/quality_scores/marginals/{}_{}_{}\".format(\n",
    "            param_dict[\"output_path\"],\n",
    "            logs[\"date_time\"],\n",
    "            CardiCat_output[\"model\"],\n",
    "            logs[\"dataset_log_name\"],\n",
    "        )\n",
    "    )\n",
    "    report_cardicat_rand[\"pairs\"].to_csv(\n",
    "        param_dict[\"lib_path\"]\n",
    "        + \"{}/quality_scores/pairs/{}_{}_{}\".format(\n",
    "            param_dict[\"output_path\"],\n",
    "            logs[\"date_time\"],\n",
    "            CardiCat_output[\"model\"],\n",
    "            logs[\"dataset_log_name\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "with open(\n",
    "    param_dict[\"lib_path\"]\n",
    "    + \"{}/reports/{}_{}_{}.json\".format(\n",
    "        param_dict[\"output_path\"],\n",
    "        logs[\"date_time\"],\n",
    "        CardiCat_output[\"model\"],\n",
    "        logs[\"dataset_log_name\"],\n",
    "    ),\n",
    "    \"w\",\n",
    ") as outfile:\n",
    "    json.dump(CardiCat_output, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f34376-4a2f-4e21-bcf3-bb83477cbb8e",
   "metadata": {},
   "source": [
    "## tVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs[\"date_time\"] = datetime.now().strftime(\"%Y%m%d_%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9332971-2997-497e-91a3-956ad5d01491",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n### Training tVAE ###\")\n",
    "tVAE_start = process_time()\n",
    "tvae = TVAE(\n",
    "    epochs=param_dict[\"epochs\"],\n",
    "    batch_size=param_dict[\"batch_size\"],\n",
    "    embedding_dim=param_dict[\"latent_dim\"],\n",
    "    compress_dims=(128, 128, 128),\n",
    "    decompress_dims=(128, 128, 128),\n",
    "    l2scale=param_dict[\"learn_rate\"],\n",
    "    loss_factor=param_dict[\"recon_factor\"],\n",
    "    field_types=postprocessing.get_metadata(train),\n",
    ")\n",
    "\n",
    "\n",
    "tvae.fit(dataframe)\n",
    "tVAE_stop = process_time()\n",
    "tVAE_time = round(tVAE_stop - tVAE_start, 2)\n",
    "print(\"Elapsed time in seconds:\", tVAE_time)\n",
    "# ## Evaluation\n",
    "tvae_gen_rand = tvae.sample(dataframe_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163b7e6-44dd-4d75-98e9-c7cf14f7dc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_tvae = postprocessing.get_report(\n",
    "    dataframe_test, tvae_gen_rand, param_dict, full=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b73d877-14c9-48ad-bfd5-da4260e27f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "anove_mixed_tvae = reporting.get_mixed_anova(catCols, intCols, floatCols, tvae_gen_rand)\n",
    "qScore_mixed_tvae, mixed_tvae = reporting.get_qScoreMixed(\n",
    "    anove_mixed_original, anove_mixed_tvae\n",
    ")\n",
    "\n",
    "cramersV_tvae = reporting.get_catCols_cramersV(catCols, tvae_gen_rand)\n",
    "score_cat_pairs_tvae, pairs_tvae = reporting.get_qScoreMixed(\n",
    "    cramersV_original, cramersV_tvae\n",
    ")\n",
    "\n",
    "(\n",
    "    ks_mixed_0_weighted_tvae,\n",
    "    ks_mixed_1_weighted_tvae,\n",
    "    ks_mixed_0_tvae,\n",
    "    ks_mixed_1_tvae,\n",
    "    ks_mixed_raw_stats_tvae,\n",
    ") = reporting.get_mean_ks_mixed_stats(\n",
    "    tvae_gen_rand,\n",
    "    dataframe_test,\n",
    "    data_encoder.catCols,\n",
    "    data_encoder.intCols,\n",
    "    data_encoder.floatCols,\n",
    ")\n",
    "\n",
    "tvae_scores = {\n",
    "    \"marginals_all\": round(report_tvae[\"summary\"].iloc[0, 1], 2),\n",
    "    \"pairs_all\": round(report_tvae[\"summary\"].iloc[1, 1], 2),\n",
    "    \"marginals_KS\": round(\n",
    "        report_tvae[\"marginals\"][report_tvae[\"marginals\"].Metric == \"KSComplement\"][\n",
    "            \"Quality Score\"\n",
    "        ].mean(),\n",
    "        2,\n",
    "    ),\n",
    "    \"marginals_TV\": round(\n",
    "        report_tvae[\"marginals\"][report_tvae[\"marginals\"].Metric == \"TVComplement\"][\n",
    "            \"Quality Score\"\n",
    "        ].mean(),\n",
    "        2,\n",
    "    ),\n",
    "    \"pairs_corr\": round(\n",
    "        report_tvae[\"pairs\"][report_tvae[\"pairs\"].Metric == \"CorrelationSimilarity\"][\n",
    "            \"Quality Score\"\n",
    "        ].mean(),\n",
    "        2,\n",
    "    ),\n",
    "    \"pairs_cont\": round(\n",
    "        report_tvae[\"pairs\"][report_tvae[\"pairs\"].Metric == \"ContingencySimilarity\"][\n",
    "            \"Quality Score\"\n",
    "        ].mean(),\n",
    "        2,\n",
    "    ),\n",
    "    \"pairs_cont_fix\": round(\n",
    "        report_tvae[\"pairs\"][\n",
    "            (report_tvae[\"pairs\"][\"Column 1\"].isin(catCols))\n",
    "            & (report_tvae[\"pairs\"][\"Column 2\"].isin(catCols))\n",
    "        ][report_tvae[\"pairs\"].Metric == \"ContingencySimilarity\"][\n",
    "            \"Quality Score\"\n",
    "        ].mean(),\n",
    "        2,\n",
    "    ),\n",
    "    \"pairs_mixed\": round(qScore_mixed_tvae, 2),\n",
    "    \"pairs_cat\": round(score_cat_pairs_tvae, 2),\n",
    "    \"pairs_ks_mixed_0_weighted\": round(\n",
    "        np.mean(list(ks_mixed_0_weighted_tvae.values())), 3\n",
    "    ),\n",
    "    \"pairs_ks_mixed_1_weighted\": round(\n",
    "        np.mean(list(ks_mixed_1_weighted_tvae.values())), 3\n",
    "    ),\n",
    "    \"pairs_ks_mixed_0\": round(np.mean(list(ks_mixed_0_tvae.values())), 3),\n",
    "    \"pairs_ks_mixed_1\": round(np.mean(list(ks_mixed_1_tvae.values())), 3),\n",
    "}\n",
    "\n",
    "kys = [\n",
    "    \"epochs\",\n",
    "    \"batch_size\",\n",
    "    \"recon_factor\",\n",
    "    \"mse_factor\",\n",
    "    \"dataset_log_name\",\n",
    "    \"latent_dim\",\n",
    "    \"date_time\",\n",
    "    \"comment\",\n",
    "    \"run_id\",\n",
    "]\n",
    "\n",
    "tVAE_output = {k: v for k, v in logs.items() if k in kys}\n",
    "tVAE_output[\"model\"] = \"tVAE\"\n",
    "tVAE_output[\"score_rand\"] = round(report_tvae[\"summary\"].Score.mean(), 2)\n",
    "tVAE_output[\"scores\"] = tvae_scores\n",
    "tVAE_output[\"marginals\"] = report_tvae[\"marginals\"][\n",
    "    [\"Column\", \"Quality Score\"]\n",
    "].to_dict()\n",
    "tVAE_output[\"pairs\"] = report_tvae[\"pairs\"][\n",
    "    [\"Column 1\", \"Column 2\", \"Metric\", \"Quality Score\"]\n",
    "].to_dict()\n",
    "tVAE_output[\"cramersV\"] = pairs_tvae.to_dict()\n",
    "tVAE_output[\"mixed\"] = mixed_tvae.to_dict()\n",
    "tVAE_output[\"mixed_ks\"] = ks_mixed_raw_stats_tvae\n",
    "tVAE_output[\"time\"] = tVAE_time\n",
    "# tVAE_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tVAE_output[\"scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba0800-afb3-4d2b-92cd-981304f6227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving synthetic pickle, and outlogs:\n",
    "if param_dict[\"save_files\"]:\n",
    "    tvae_gen_rand.to_pickle(\n",
    "        os.path.join(\n",
    "            param_dict[\"lib_path\"],\n",
    "            \"{}/synthetics/synthetics_pkl_{}_{}_{}\".format(\n",
    "                param_dict[\"output_path\"],\n",
    "                logs[\"dataset_log_name\"],\n",
    "                logs[\"date_time\"],\n",
    "                \"tVAE\" + \"_rand\",\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "outlog_tvae = pd.DataFrame.from_dict({key: [tVAE_output[key]] for key in tVAE_output})\n",
    "if param_dict[\"save_files\"]:\n",
    "    outlog_tvae.to_csv(\n",
    "        param_dict[\"lib_path\"]\n",
    "        + \"{}/outlogs/{}_{}_{}\".format(\n",
    "            param_dict[\"output_path\"],\n",
    "            logs[\"date_time\"],\n",
    "            tVAE_output[\"model\"],\n",
    "            logs[\"dataset_log_name\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "with open(\n",
    "    param_dict[\"lib_path\"]\n",
    "    + \"{}/reports/{}_{}_{}.json\".format(\n",
    "        param_dict[\"output_path\"],\n",
    "        logs[\"date_time\"],\n",
    "        tVAE_output[\"model\"],\n",
    "        logs[\"dataset_log_name\"],\n",
    "    ),\n",
    "    \"w\",\n",
    ") as outfile:\n",
    "    json.dump(tVAE_output, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3c33cd-e8fe-4706-a36c-9bf31a556136",
   "metadata": {},
   "source": [
    "## tGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580072f8-b04a-4dcf-a231-85c2f5377abc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n### Training tGAN ###\")\n",
    "tGAN_start = process_time()\n",
    "tGAN = CTGAN(\n",
    "    epochs=param_dict[\"epochs\"],\n",
    "    batch_size=param_dict[\"batch_size\"],\n",
    "    embedding_dim=param_dict[\"latent_dim\"],\n",
    "    generator_dim=(128, 128, 128),\n",
    "    discriminator_dim=(128, 128, 128),\n",
    "    verbose=True,\n",
    "    field_types=postprocessing.get_metadata(train),\n",
    "    generator_lr=param_dict[\"learn_rate\"],\n",
    "    discriminator_lr=param_dict[\"learn_rate\"],\n",
    ")\n",
    "\n",
    "\n",
    "# tvae.fit(dataframe.drop('target',axis=1))\n",
    "tGAN.fit(dataframe)\n",
    "tGAN_stop = process_time()\n",
    "tGAN_time = round(tGAN_stop - tGAN_start, 2)\n",
    "print(\"Elapsed time in seconds:\", tGAN_time)\n",
    "# ## Evaluation\n",
    "tGAN_gen_rand = tGAN.sample(dataframe_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc4518-a3ff-4e29-98e3-66f33de88b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_tgan = postprocessing.get_report(\n",
    "    dataframe_test, tGAN_gen_rand, param_dict, full=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anove_mixed_tgan = reporting.get_mixed_anova(catCols, intCols, floatCols, tGAN_gen_rand)\n",
    "qScore_mixed_tgan, mixed_tgan = reporting.get_qScoreMixed(\n",
    "    anove_mixed_original, anove_mixed_tgan\n",
    ")\n",
    "\n",
    "cramersV_tgan = reporting.get_catCols_cramersV(catCols, tGAN_gen_rand)\n",
    "score_cat_pairs_tgan, pairs_tgan = reporting.get_qScoreMixed(\n",
    "    cramersV_original, cramersV_tgan\n",
    ")\n",
    "\n",
    "(\n",
    "    ks_mixed_0_weighted_tgan,\n",
    "    ks_mixed_1_weighted_tgan,\n",
    "    ks_mixed_0_tgan,\n",
    "    ks_mixed_1_tgan,\n",
    "    ks_mixed_raw_stats_tgan,\n",
    ") = reporting.get_mean_ks_mixed_stats(\n",
    "    tGAN_gen_rand,\n",
    "    dataframe_test,\n",
    "    data_encoder.catCols,\n",
    "    data_encoder.intCols,\n",
    "    data_encoder.floatCols,\n",
    ")\n",
    "\n",
    "tgan_scores = {\n",
    "    \"marginals_all\": round(report_tgan[\"summary\"].iloc[0, 1], 2),\n",
    "    \"pairs_all\": round(report_tgan[\"summary\"].iloc[1, 1], 2),\n",
    "    \"marginals_KS\": round(\n",
    "        report_tgan[\"marginals\"][report_tgan[\"marginals\"].Metric == \"KSComplement\"][\n",
    "            \"Quality Score\"\n",
    "        ].mean(),\n",
    "        2,\n",
    "    ),\n",
    "    \"marginals_TV\": round(\n",
    "        report_tgan[\"marginals\"][report_tgan[\"marginals\"].Metric == \"TVComplement\"][\n",
    "            \"Quality Score\"\n",
    "        ].mean(),\n",
    "        2,\n",
    "    ),\n",
    "    \"pairs_corr\": round(\n",
    "        report_tgan[\"pairs\"][report_tgan[\"pairs\"].Metric == \"CorrelationSimilarity\"][\n",
    "            \"Quality Score\"\n",
    "        ].mean(),\n",
    "        2,\n",
    "    ),\n",
    "    \"pairs_cont\": round(\n",
    "        report_tgan[\"pairs\"][report_tgan[\"pairs\"].Metric == \"ContingencySimilarity\"][\n",
    "            \"Quality Score\"\n",
    "        ].mean(),\n",
    "        2,\n",
    "    ),\n",
    "    \"pairs_cont_fix\": round(\n",
    "        report_tvae[\"pairs\"][\n",
    "            (report_tvae[\"pairs\"][\"Column 1\"].isin(catCols))\n",
    "            & (report_tvae[\"pairs\"][\"Column 2\"].isin(catCols))\n",
    "        ][report_tvae[\"pairs\"].Metric == \"ContingencySimilarity\"][\n",
    "            \"Quality Score\"\n",
    "        ].mean(),\n",
    "        2,\n",
    "    ),\n",
    "    \"pairs_mixed\": round(qScore_mixed_tgan, 2),\n",
    "    \"pairs_cat\": round(score_cat_pairs_tgan, 2),\n",
    "    \"pairs_ks_mixed_0_weighted\": round(\n",
    "        np.mean(list(ks_mixed_0_weighted_tgan.values())), 3\n",
    "    ),\n",
    "    \"pairs_ks_mixed_1_weighted\": round(\n",
    "        np.mean(list(ks_mixed_1_weighted_tgan.values())), 3\n",
    "    ),\n",
    "    \"pairs_ks_mixed_0\": round(np.mean(list(ks_mixed_0_tgan.values())), 3),\n",
    "    \"pairs_ks_mixed_1\": round(np.mean(list(ks_mixed_1_tgan.values())), 3),\n",
    "}\n",
    "\n",
    "kys = [\n",
    "    \"epochs\",\n",
    "    \"batch_size\",\n",
    "    \"recon_factor\",\n",
    "    \"mse_factor\",\n",
    "    \"dataset_log_name\",\n",
    "    \"latent_dim\",\n",
    "    \"date_time\",\n",
    "    \"comment\",\n",
    "    \"run_id\",\n",
    "]\n",
    "\n",
    "tGAN_output = {k: v for k, v in logs.items() if k in kys}\n",
    "tGAN_output[\"model\"] = \"tGAN\"\n",
    "tGAN_output[\"score_rand\"] = round(report_tgan[\"summary\"].Score.mean(), 2)\n",
    "tGAN_output[\"scores\"] = tgan_scores\n",
    "tGAN_output[\"marginals\"] = report_tgan[\"marginals\"][\n",
    "    [\"Column\", \"Quality Score\"]\n",
    "].to_dict()\n",
    "tGAN_output[\"pairs\"] = report_tgan[\"pairs\"][\n",
    "    [\"Column 1\", \"Column 2\", \"Metric\", \"Quality Score\"]\n",
    "].to_dict()\n",
    "tGAN_output[\"cramersV\"] = pairs_tgan.to_dict()\n",
    "tGAN_output[\"mixed\"] = mixed_tgan.to_dict()\n",
    "tGAN_output[\"mixed_ks\"] = ks_mixed_raw_stats_tgan\n",
    "tGAN_output[\"time\"] = tGAN_time\n",
    "# tGAN_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tGAN_output[\"scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca67959-f10f-4a1f-ab6e-ba18470ee3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if param_dict[\"save_files\"]:\n",
    "    tGAN_gen_rand.to_pickle(\n",
    "        os.path.join(\n",
    "            param_dict[\"lib_path\"],\n",
    "            \"{}/synthetics/synthetics_pkl_{}_{}_{}\".format(\n",
    "                param_dict[\"output_path\"],\n",
    "                logs[\"dataset_log_name\"],\n",
    "                logs[\"date_time\"],\n",
    "                \"tGAN\" + \"_rand\",\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "outlog_tgan = pd.DataFrame.from_dict({key: [tGAN_output[key]] for key in tGAN_output})\n",
    "if param_dict[\"save_files\"]:\n",
    "    outlog_tgan.to_csv(\n",
    "        param_dict[\"lib_path\"]\n",
    "        + \"{}/outlogs/{}_{}_{}\".format(\n",
    "            param_dict[\"output_path\"],\n",
    "            logs[\"date_time\"],\n",
    "            tGAN_output[\"model\"],\n",
    "            logs[\"dataset_log_name\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "with open(\n",
    "    param_dict[\"lib_path\"]\n",
    "    + \"{}/reports/{}_{}_{}.json\".format(\n",
    "        param_dict[\"output_path\"],\n",
    "        logs[\"date_time\"],\n",
    "        tGAN_output[\"model\"],\n",
    "        logs[\"dataset_log_name\"],\n",
    "    ),\n",
    "    \"w\",\n",
    ") as outfile:\n",
    "    json.dump(tGAN_output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af8b1d2-09d6-4567-b2ba-62a737dc9e78",
   "metadata": {},
   "source": [
    "### END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CardiCat_neurips",
   "language": "python",
   "name": "cardicat_neurips"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
