#########################
#### CardiCat ###########
#########################


import numpy as np
import pandas as pd
from sdmetrics.reports.single_table import QualityReport


def cumulative(lst):
    """Given a list of numerical elements, returns the cumulative sum.

    Parameters
    ----------
    lst : list
            The list that will be used to calculate the cumulative sum.

    Example:
    lst = [1,2,3,4]
    cumulative(lst) --> [1,3,6,10]
    """
    cu_list = []
    length = len(lst)
    cu_list = [sum(lst[0:x:1]) for x in range(0, length + 1)]
    return cu_list[1:]


def partition(alist, indices):
    """Given a list (alist), returns a list of lists based on the start:end positions of a list indices.

    Parameters
    ----------
    alist : list
            The list that needs to be broken to sub-lists.
    indices : list
            The list that indicates the start:end indices to break the list.

    Example:
    alist = [1,2,3,4,5,6];indices=[1,3,5,6]
    postprocessing.partition(alist,indices) --> [[1], [2, 3], [4, 5], [6]]
    """
    return [alist[i:j] for i, j in zip([0] + indices, indices)]


def get_pred(
    enc,
    dec,
    latents,
    param_dict,
    oh_tokens,
    emb_sizes,
    embCols,
    numFeatures,
    catCols,
    numCols,
    numLookup,
    intCols,
    all_inputs,
    # all_inputs_1hot,
    col_tokens_all,
    label_encoder,
):
    """Generating synthetic data using the decoder model, and latents as input.

    Args:
        enc (tf.model): _description_
        dec (tf.model): _description_
        latents (_type_): _description_
        param_dict (_type_): _description_
        oh_tokens (_type_): _description_
        emb_sizes (_type_): _description_
        embCols (_type_): _description_
        numFeatures (_type_): _description_
        catCols (_type_): _description_
        numCols (_type_): _description_
        numLookup (_type_): _description_
        intCols (_type_): _description_
        all_inputs (_type_): _description_
        col_tokens_all (_type_): _description_
        label_encoder (_type_): _description_

    Returns:
        pd.DataFrame: the synthetic data generated by inputing the latens into the decoder.
    """

    generated_x = dec.predict(latents)
    # for some datasets, this is necessary:
    if len(generated_x) == 2:
        generated_x = [np.append(i, j) for i, j in zip(generated_x[0], generated_x[1])]
    # decoding / denormalization the encoded generated format to match original:
    ## Getting layer sizes to apply decoding:
    if param_dict["emb_loss"]:
        layer_sizes = oh_tokens + emb_sizes + [i.shape[1] for i in numFeatures]
    else:
        layer_sizes = col_tokens_all + [i.shape[1] for i in numFeatures]
    layer_indx = cumulative(layer_sizes)
    # Partitioning the generated data into input layers:
    gen_x = [partition(i, layer_indx) for i in generated_x]
    print("Finished partitioning Generated Data")
    # Getting the label (nominal) of 1hot features:
    if param_dict["emb_loss"]:
        gen_x_1hot = [[np.argmax(i) for i in j[: len(oh_tokens)]] for j in gen_x]
    else:
        gen_x_1hot = [[np.argmax(i) for i in j[: len(col_tokens_all)]] for j in gen_x]
    print("Finished Translating 1hot columns to integers")
    # Getting the label   of embedded features:
    if param_dict["emb_loss"]:
        emb_weights = {}
        for layer in enc.layers:
            if (
                layer.name.startswith("emb_")
                and hasattr(layer, "trainable_weights")
                and layer.trainable_weights
            ):
                emb_weights[layer.name] = layer.trainable_weights[0].numpy()
        gen_x_emb = [
            [
                (
                    np.sum(
                        np.square(
                            emb_weights["emb_" + e]
                            - [k] * emb_weights["emb_" + e].shape[0]
                        ),
                        axis=1,
                    )
                    / emb_weights["emb_" + e].shape[1]
                ).argmin()  # find the emb index closest
                for k, e in zip(
                    j[len(oh_tokens) : len(oh_tokens) + len(emb_sizes)], embCols
                )
            ]
            for j in gen_x
        ]
        print("Finished Translating embedded columns to integers")

    ## Denormalizing numerical variables;
    gen_x_num = [[i[0] for i in j[-len(numCols) :]] for j in gen_x]
    ## BUilding a final dataset with all types decoded:
    if param_dict["emb_loss"]:
        gen_x_all = [
            hot + emb + num for hot, emb, num in zip(gen_x_1hot, gen_x_emb, gen_x_num)
        ]
    else:
        gen_x_all = [hot + num for hot, num in zip(gen_x_1hot, gen_x_num)]
    # Translating/Decoding the output into the original form:
    if param_dict["emb_loss"]:
        gen_df = pd.DataFrame(gen_x_all, columns=[t.name for t in all_inputs])
    else:
        gen_df = pd.DataFrame(gen_x_all, columns=[t for t in catCols + numCols])

    for i in label_encoder:
        gen_df[i] = label_encoder[i].inverse_transform(gen_df[i])
    if numLookup:
        for i, j in zip(numCols, numLookup):
            gen_df[i] = de_normalizer(gen_df[i], j)
    for k in intCols:
        gen_df[k] = gen_df[k].round(0).astype("int")
    if param_dict["emb_loss"]:
        return gen_df, emb_weights
    else:
        return gen_df, []


def get_metadata(dataframe):
    """Get the types metadata of a dataframe

    Args:
        dataframe (pd.DataFrame): A pandas dataframe

    Returns:
        dict: a dict with type metadata information
    """
    tmp = {}
    metadata = {}
    for i in dataframe.columns:
        if dataframe.dtypes[i] == "object":
            tmp[i] = {"type": "categorical"}
        elif dataframe.dtypes[i] == "float64":
            tmp[i] = {"type": "numerical", "subtype": "float"}
        elif (dataframe.dtypes[i] == "int64") or (
            dataframe.dtypes[i] == pd.Int64Dtype()
        ):
            tmp[i] = {"type": "numerical", "subtype": "int"}
        else:
            print("Didn't match on any data type")
    metadata["fields"] = tmp
    return metadata


def get_report(X, Xhat, param_dict, full=False):
    """Returns a quality report between X and Xhat.

    Args:
        X (pd.DataFrame): original dataframe
        Xhat (pd.DataFrame): synthetic dataframe
        param_dict (dict): if there is a target label we need to ignore
        full (bool, optional): Returns a full report. Defaults to False.

    Returns:
        list: A quality report between X and Xhat
    """
    report = QualityReport()
    if param_dict["is_target"]:
        X = X[X.drop("target", axis=1).columns.sort_values()]
    else:
        X = X[X.columns.sort_values()]

    report.generate(X, Xhat[X.columns], get_metadata(X))
    if full:
        return {
            "summary": report.get_properties(),
            "marginals": report.get_details(property_name="Column Shapes"),
            "pairs": report.get_details(property_name="Column Pair Trends"),
        }
    else:
        return report.get_score()


def decode_splits(Xsplit, label_encoder):
    """Applying inverse transform operation to get original labels from label encoder.

    Args:
        Xsplit (array): data to be inverse transform
        label_encoder (sklearn.preprocessing.LabelEncoder): a label encoder object

    Returns:
        array: The inverse transformed array
    """
    for i in label_encoder:
        Xsplit[i] = label_encoder[i].inverse_transform(Xsplit[i])
    return Xsplit


def de_normalizer(x, numLookup):
    """Denormalize (scale-shift) the numerical column x based on a numLookup

    Args:
        x (numpy.array): an array to de-normalize
        numLookup (keras.layers.Normalization): a Normalization object

    Returns:
        numpy.array: a de-normalized array.
    """
    return x * np.sqrt(numLookup.variance[0][0].numpy()) + numLookup.mean[0][0].numpy()
